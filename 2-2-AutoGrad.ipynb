{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap1-2: 自动求导\n",
    "这次课程我们会了解 PyTorch 中的自动求导机制，自动求导是 PyTorch 中非常重要的特性，能够让我们避免手动去计算非常复杂的导数，这能够极大地减少了我们构建模型的时间，这也是其前身 Torch 这个框架所不具备的特性，下面我们通过例子看看 PyTorch 自动求导的独特魅力以及探究自动求导的更多用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 简单情况下的求导, 对标量结果进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.], requires_grad=True)\n",
      "tensor([19.], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "x=Variable(torch.Tensor([2]),requires_grad=True)\n",
    "y=x+2\n",
    "z=y**2+3\n",
    "print(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### z对x进行求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上面的一些列操作，我们从 x 得到了最后的结果out，我们可以将其表示为数学公式\n",
    "\n",
    "$$\n",
    "z = (x + 2)^2 + 3\n",
    "$$\n",
    "那么我们从 z 对 x 求导的结果就是\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = 2 (x + 2) = 2 (2 + 2) = 8\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 再来一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2902, grad_fn=<MeanBackward1>)\n",
      "对x求导 tensor([[ 0.1071, -0.2666,  0.5740],\n",
      "        [ 0.1071, -0.2666,  0.5740]])\n",
      "对y求导 tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "对w求导 tensor([[ 0.1787,  0.1787],\n",
      "        [-0.1115, -0.1115],\n",
      "        [ 0.1440,  0.1440]])\n"
     ]
    }
   ],
   "source": [
    "x=Variable(torch.randn(2,3),requires_grad=True)\n",
    "w=Variable(torch.randn(3,2),requires_grad=True)\n",
    "y=Variable(torch.randn(2,2),requires_grad=True)\n",
    "out=torch.mean(y- torch.matmul(x,w))\n",
    "out.backward()\n",
    "#得到x的梯度\n",
    "print(out)\n",
    "print('对x求导',x.grad)\n",
    "print('对y求导',y.grad)\n",
    "print('对w求导',w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 扩展:\n",
    "- 矩阵乘法:`torch.matmul(tensor1,tensor2)`, \n",
    "- 求均值:`torch.mean(tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([2, 1])\n",
      "tensor([[8.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵相乘\n",
    "tensor1=torch.Tensor([[1,2]])\n",
    "tensor2=torch.Tensor([[2],[3]])\n",
    "print(tensor1.shape)\n",
    "print(tensor2.shape)\n",
    "print(torch.matmul(tensor1,tensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([2., 5.])\n",
      "tensor([2.5000, 3.5000, 4.5000])\n"
     ]
    }
   ],
   "source": [
    "# 求均值\n",
    "tensor=torch.Tensor([[1,2,3],[4,5,6]])\n",
    "print(tensor.shape)\n",
    "print(torch.mean(tensor,dim=-1))\n",
    "print(torch.mean(tensor,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. 复杂情况下的自动求导, 对向量或矩阵结果自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始n:\n",
      " tensor([[0., 0.]])\n",
      "新n:\n",
      " tensor([[ 4., 27.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "m=Variable(torch.FloatTensor([[2,3]]),requires_grad=True)\n",
    "n=Variable(torch.zeros(1,2))\n",
    "print('原始n:\\n',n)\n",
    "n[0,0]=m[0,0]**2\n",
    "n[0,1]=m[0,1]**3\n",
    "print('新n:\\n',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 PyTorch 中，如果要调用自动求导，需要往backward()中传入**一个参数**，这个参数的形状和 n 一样大，比如是 $(w_0,\\ w_1)$那么自动求导的结果就是： \n",
    "$$\n",
    "\\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4., 27.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵n对m进行求导,向backward()中传入一个tensor\n",
    "n.backward(torch.ones_like(n))\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过自动求导我们得到了梯度是 4 和 27，我们可以验算一下 $$\n",
    "\\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0} = 2 m_0 + 0 = 2 \\times 2 = 4\n",
    "$$$$\n",
    "\\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1} = 0 + 3 m_1^2 = 3 \\times 3^2 = 27\n",
    "$$ 通过验算我们可以得到相同的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三. 多次自动求导\n",
    "通过调用 backward 我们可以进行一次自动求导，如果我们再调用一次 backward，会发现程序报错，没有办法再做一次。这是因为 PyTorch 默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，我们通过下面的小例子来说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.], grad_fn=<AddBackward>)\n"
     ]
    }
   ],
   "source": [
    "x=Variable(torch.FloatTensor([3]),requires_grad=True) # 设置retain_graph 为 True 来保留计算图\n",
    "y=x*2+x**2+3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "y.backward(retain_graph=True) #设置retain_graph为True来保留计算图\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现 x 的梯度变成了 16，因为这里做了两次自动求导，所以讲第一次的梯度 8 和第二次的梯度 8 加起来得到了 16 的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习**\n",
    "\n",
    "定义\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x_0 \\\\\n",
    "x_1\n",
    "\\end{matrix}\n",
    "\\right] = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2 \\\\\n",
    "3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "k = (k_0,\\ k_1) = (x_0^2 + 3 x_1,\\ 2 x_0 + x_1^2)\n",
    "$$\n",
    "\n",
    "我们希望求得\n",
    "\n",
    "$$\n",
    "j = \\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial k_0}{\\partial x_0} & \\frac{\\partial k_0}{\\partial x_1} \\\\\n",
    "\\frac{\\partial k_1}{\\partial x_0} & \\frac{\\partial k_1}{\\partial x_1}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "参考答案：\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "4 & 3 \\\\\n",
    "2 & 6 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 3.])\n"
     ]
    }
   ],
   "source": [
    "x=Variable(torch.Tensor([2,3]),requires_grad=True)\n",
    "k=Variable(torch.zeros(2))\n",
    "\n",
    "k[0]=x[0]**2+3*x[1]\n",
    "k[1]=x[0]*2+x[1]**2\n",
    "\n",
    "k.backward(torch.FloatTensor([1, 0]), retain_graph=True) # [1,0]是对应下边的公式\n",
    "print(x.grad)#对应答案第一行\n",
    "\n",
    "answer=torch.zeros(2,2) #保存到j\n",
    "answer[0]=x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial k}{\\partial x_0} = w_0 \\frac{\\partial k_0}{\\partial x_0} + w_1 \\frac{\\partial k_1}{\\partial x_0}\n",
    "=1*2x_0+0*2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial k}{\\partial x_1} = w_0 \\frac{\\partial k_0}{\\partial x_1} + w_1 \\frac{\\partial k_1}{\\partial x_1}\n",
    "=1*3+0*2x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [2., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_() #inplace操作, 将之前求得的梯度归零\n",
    "k.backward(torch.FloatTensor([0,1])) #此处的[0,1]对应下边的公式:\n",
    "j[1]=x.grad.data\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial k}{\\partial x_0} = w_0 \\frac{\\partial k_0}{\\partial x_0} + w_1 \\frac{\\partial k_1}{\\partial x_0}\n",
    "=0*2x_0+1*2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial k}{\\partial x_1} = w_0 \\frac{\\partial k_0}{\\partial x_1} + w_1 \\frac{\\partial k_1}{\\partial x_1}\n",
    "=0*3+1*2x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch语法小结\n",
    "- `torch.mean(tensor, dim=)`:求均值\n",
    "- `torch.matmul(tensor1,tensor2)`:求矩阵乘法\n",
    "- `torch.ones_like(tensor)`:得到全1矩阵,注意下划线\\_\n",
    "- `tensor.zero_()`:将矩阵设为全0矩阵,inplace操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
