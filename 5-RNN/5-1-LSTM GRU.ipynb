{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 中的循环神经网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.`RNN()`:\n",
    "- input_size 表示输入 $x_t$ 的特征维度\n",
    "\n",
    "- hidden_size 表示输出的特征维度\n",
    "\n",
    "- num_layers 表示网络的层数\n",
    "\n",
    "- nonlinearity 表示选用的非线性激活函数，默认是 'tanh'\n",
    "\n",
    "- bias 表示是否使用偏置，默认使用\n",
    "\n",
    "- batch_first 表示输入数据的形式，默认是 False，就是这样形式，(seq, batch, feature)，也就是将序列长度放在第一位，batch 放在第二位\n",
    "\n",
    "- dropout 表示是否在输出层应用 dropout\n",
    "\n",
    "- bidirectional 表示是否使用双向的 rnn，默认是 False\n",
    "\n",
    "### 2.`RNNCell()`:\n",
    "- input_size\n",
    "- hidden_size\n",
    "- bias\n",
    "- nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.使用nn.RNNCell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个单步的rnn\n",
    "rnn_single=nn.RNNCell(input_size=100,hidden_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "torch.Size([200, 200])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623, -0.0017, -0.0243,  ...,  0.0045, -0.0348, -0.0245],\n",
      "        [-0.0536,  0.0099, -0.0676,  ...,  0.0231,  0.0624,  0.0654],\n",
      "        [ 0.0546,  0.0631,  0.0382,  ..., -0.0012,  0.0456,  0.0353],\n",
      "        ...,\n",
      "        [-0.0089, -0.0003,  0.0315,  ...,  0.0249, -0.0190, -0.0662],\n",
      "        [ 0.0426, -0.0568, -0.0666,  ...,  0.0310, -0.0290, -0.0634],\n",
      "        [ 0.0368, -0.0443,  0.0068,  ...,  0.0241,  0.0451,  0.0433]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#访问其中的参数\n",
    "a=rnn_single.weight_hh\n",
    "print(type(a))\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个序列，长为 6，batch 是 5， 特征是 100\n",
    "x=Variable(torch.randn(6,5,100))#sequence=6,batch=5,input_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义初始的记忆状态\n",
    "h_t=Variable(torch.zeros(5,200))#batch=5,hidden_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#传入rnn,模拟了rnn的迭代过程\n",
    "out=[]\n",
    "for i in range(6):\n",
    "    h_t=rnn_single(x[i],h_t)#把输入tensor和初始启动记忆tensor\n",
    "    out.append(h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 200])\n"
     ]
    }
   ],
   "source": [
    "#输出最后的state tensor\n",
    "print(h_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#看一下out\n",
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 200])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.使用nn.RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_seq=nn.RNN(100,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0449,  0.0621,  0.0357,  ...,  0.0145,  0.0256, -0.0150],\n",
      "        [ 0.0219,  0.0363, -0.0539,  ...,  0.0149,  0.0189, -0.0651],\n",
      "        [-0.0138, -0.0648, -0.0410,  ..., -0.0561,  0.0216,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0328, -0.0165,  0.0406,  ...,  0.0408, -0.0678, -0.0704],\n",
      "        [ 0.0016,  0.0201, -0.0578,  ..., -0.0404, -0.0566, -0.0480],\n",
      "        [ 0.0681, -0.0066, -0.0701,  ...,  0.0458,  0.0673,  0.0132]],\n",
      "       requires_grad=True)\n",
      "torch.Size([200, 200])\n"
     ]
    }
   ],
   "source": [
    "#访问其中的参数\n",
    "a=rnn_seq.weight_hh_l0\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, h_t = rnn_seq(x) # 使用默认的全 0 隐藏状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 5, 200])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([6, 5, 200])\n"
     ]
    }
   ],
   "source": [
    "a=h_t\n",
    "print(type(a))\n",
    "print(a.shape)\n",
    "print(type(out))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己定义初始的隐藏状态\n",
    "h_0 = Variable(torch.randn(1, 5,\n",
    "                           200)) # 这里的隐藏状态的大小有三个维度，分别是 (num_layers * num_direction, batch, hidden_size)\n",
    "out, h_t = rnn_seq(x, h_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 200])\n"
     ]
    }
   ],
   "source": [
    "a=h_t\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5, 200])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时输出的结果也是 (seq, batch, feature)\n",
    "\n",
    "一般情况下我们都是用 **nn.RNN()** 而不是 nn.RNNCell()，因为 nn.RNN() 能够避免我们手动写循环，非常方便，同时如果不特别说明，我们也会选择使用默认的全 0 初始化隐藏状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. LSTM\n",
    "LSTM 和基本的 RNN 是一样的，他的参数也是相同的，同时他也有 nn.LSTMCell() 和 nn.LSTM() 两种形式，跟前面讲的都是相同的，我们就不再赘述了，下面直接举个小例子\n",
    "### 1.nn.LSTM(input_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_seq=nn.LSTM(50,100,num_layers=2) # 输入维度 100，输出 200，两层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 100])\n"
     ]
    }
   ],
   "source": [
    "a=lstm_seq.weight_hh_l0 #第一层的h_t权重\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input=Variable(torch.randn(10,3,50))#序列10, batch=3,输入50\n",
    "out,(h,c)=lstm_seq(lstm_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里 LSTM 输出的隐藏状态有两个，h 和 c，就是上图中的每个 cell 之间的两个箭头，这两个隐藏状态的大小都是相同的，(num_layers * direction, batch, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape # 两层, Batch是3,特征是100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 100])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以不使用默认的隐藏状态，这是需要传入两个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_init=Variable(torch.randn(2,3,100))#shape的定义是[num_layers,batch,output_size]\n",
    "c_init=Variable(torch.randn(2,3,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,(h,c)=lstm_seq(lstm_input,(h_init,c_init))#在MT中seq2seq中,这句话是我们需要用到的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 100])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_seq=nn.GRU(10,20)\n",
    "gru_input=Variable(torch.randn(3,32,10))\n",
    "out,h=gru_seq(gru_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0858,  0.1167, -0.2150,  ...,  0.1816,  0.0614, -0.0686],\n",
      "        [ 0.0204,  0.1537,  0.1798,  ..., -0.1466,  0.0764, -0.2056],\n",
      "        [ 0.1458, -0.0388,  0.0597,  ..., -0.1942, -0.0172, -0.0745],\n",
      "        ...,\n",
      "        [ 0.2145, -0.0718,  0.1385,  ..., -0.1296,  0.2046,  0.0909],\n",
      "        [ 0.2044,  0.2040, -0.0603,  ..., -0.1046, -0.2156, -0.1384],\n",
      "        [ 0.1613,  0.1350, -0.0216,  ...,  0.1015,  0.0816, -0.0669]],\n",
      "       requires_grad=True)\n",
      "torch.Size([60, 20])\n"
     ]
    }
   ],
   "source": [
    "a=gru_seq.weight_hh_l0\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 20])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 20])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结:\n",
    "- nn.LSTM(input_size,output_size,num_layers)\n",
    "- nn.GRU(input_size, output_size,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
