{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分自动编码器-正态分布隐向量自动编码器\n",
    "变分编码器是自动编码器的升级版本，其结构跟自动编码器是类似的，也由编码器和解码器构成。\n",
    "\n",
    "回忆一下，自动编码器有个问题，就是并不能任意生成图片，因为我们没有办法自己去构造隐藏向量，需要通过一张图片输入编码我们才知道得到的隐含向量是什么，这时我们就可以通过变分自动编码器来解决这个问题。\n",
    "\n",
    "其实原理特别简单，只需要在编码过程给它增加一些限制，迫使其生成的隐含向量能够粗略的遵循一个标准正态分布，这就是其与一般的自动编码器最大的不同。\n",
    "\n",
    "这样我们生成一张新图片就很简单了，我们只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片先编码。\n",
    "\n",
    "一般来讲，我们通过 encoder 得到的隐含向量并不是一个标准的正态分布，为了衡量两种分布的相似程度，我们使用 **KL divergence(KL散度,相对熵)**，利用其来表示隐含向量与标准正态分布之间差异的 loss，另外一个 loss 仍然使用生成图片与原图片的均方误差来表示。\n",
    "\n",
    "KL divergence 的公式如下\n",
    "\n",
    "$$\n",
    "D{KL} (P || Q) =  \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重参数\n",
    "为了避免计算 KL divergence 中的积分，我们使用重参数的技巧，不是每次产生一个隐含向量，而是生成两个向量，一个表示均值，一个表示标准差，这里我们默认编码之后的隐含向量服从一个正态分布的之后，就可以用一个标准正态分布先乘上标准差再加上均值来合成这个正态分布，最后 loss 就是希望这个生成的正态分布能够符合一个标准正态分布，也就是希望均值为 0，方差为 1\n",
    "\n",
    "所以标准的变分自动编码器如下\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tKfTcgy1fn15cq6n7pj30k007t0sv.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们用 mnist 数据集来简单说明一下变分自动编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as tfs\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_tfs=tfs.Compose([tfs.ToTensor(),\n",
    "                   tfs.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])]) #标准化\n",
    "train_set=MNIST('./mnist',transform=im_tfs)\n",
    "train_data=DataLoader(train_set,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE,self).__init__()\n",
    "        self.fc1=nn.Linear(784,400)\n",
    "        self.fc21=nn.Linear(400,20) #mean均值\n",
    "        self.fc22=nn.Linear(400,20) #var标准差\n",
    "        self.fc3=nn.Linear(20,400)\n",
    "        self.fc4=nn.Linear(400,784)\n",
    "    def encode(self,x):\n",
    "        h1=F.relu(self.fc1(x))\n",
    "        return self.fc21(h1),self.fc22(h1)\n",
    "    def reparametrize(self,mu,logvar):\n",
    "        \n",
    "        std=logvar.mul(0.5).exp_()#乘以0.5然后平方\n",
    "        eps=torch.FloatTensor(std.size()).normal_() #生成一个基础向量\n",
    "        if torch.cuda.is_available():\n",
    "            eps=Variable(eps.cuda())\n",
    "        else:\n",
    "            eps=Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    def decode(self,z):\n",
    "        h3=F.relu(self.fc3(z))\n",
    "        h4=F.tanh(self.fc4(h3))\n",
    "        return h4\n",
    "    def forward(self,x):\n",
    "        mu,logvar=self.encode(x) #编码\n",
    "        z=self.reparametrize(mu,logvar)\n",
    "        out=self.decode(z)\n",
    "        return out,mu,logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=VAE()#实例化网络\n",
    "if torch.cuda.is_available():\n",
    "    net=net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\APP\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "x,_=train_set[0]\n",
    "x=x.view(x.shape[0],-1)\n",
    "if torch.cuda.is_available():\n",
    "    x=x.cuda()\n",
    "x=Variable(x)\n",
    "_,mu,var=net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1939,  0.0716,  0.0595, -0.1389,  0.0224, -0.3021, -0.1536,  0.0457,\n",
      "          0.1008,  0.2662,  0.1975,  0.1385,  0.1655, -0.1013, -0.2925,  0.0364,\n",
      "          0.3578, -0.0608,  0.0093, -0.0272]],\n",
      "       device='cuda:0', grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，对于输入，网络可以输出隐含变量的均值和方差，这里的均值方差还没有训练\n",
    "\n",
    "下面开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reconstruction_function = nn.MSELoss(reduction='sum')\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    recon_x: generating images 原始图片\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    MSE = reconstruction_function(recon_x, x)\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return MSE + KLD\n",
    "\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=1e-3)\n",
    "\n",
    "def to_img(x):\n",
    "    #将最后的结果转化为图片\n",
    "    x=0.5*(x+1.0)\n",
    "    x=x.clamp(0,1)\n",
    "    x=x.view(x.shape[0],1,28,28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\APP\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20,Loss:66.1061\n",
      "epoch:40,Loss:62.9825\n",
      "epoch:60,Loss:63.3907\n",
      "epoch:80,Loss:62.2309\n",
      "epoch:100,Loss:62.7153\n"
     ]
    }
   ],
   "source": [
    "for e in range(100):\n",
    "    for im,_ in train_data:\n",
    "        im=im.view(im.shape[0],-1)\n",
    "        im=Variable(im)\n",
    "        if torch.cuda.is_available():\n",
    "            im=im.cuda()\n",
    "        recon_im,mu,logvar=net(im)\n",
    "        loss=loss_function(recon_im,im,mu,logvar)/im.shape[0] # 将 loss 平均\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (e+1)%20==0:\n",
    "        print('epoch:{},Loss:{:.4f}'.format(e+1,loss.item()))\n",
    "        save=to_img(recon_im.cpu().data)\n",
    "        if not os.path.exists('./vae_img'):\n",
    "            os.mkdir('./vae_img')\n",
    "        save_image(save,'./vae_img/image_{}.png'.format(e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
