{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10介绍\n",
    "cifar 10 这个数据集一共有 50000 张训练集，10000 张测试集，两个数据集里面的图片都是 png 彩色图片，图片大小是 32 x 32 x 3，一共是 10 分类问题，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。这个数据集是对网络性能测试一个非常重要的指标，可以说如果一个网络在这个数据集上超过另外一个网络，那么这个网络性能上一定要比另外一个网络好，目前这个数据集最好的结果是 95% 左右的测试集准确率。\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpjxxq7wcj30db0ae7ag.jpg)\n",
    "cifar 10 已经被 pytorch 内置了，使用非常方便，只需要调用 `torchvision.datasets.CIFAR10` 就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet介绍\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军，得益于 python 的函数和循环，我们能够非常方便地构建重复结构的深层网络。\n",
    "\n",
    "vgg 的网络结构非常简单，就是不断地堆叠卷积层和池化层，下面是一个简单的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpk5smtidj307n0dx3yv.jpg)\n",
    "\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    net=[nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1),nn.ReLU(True)] #定义第一层\n",
    "    for i in range(num_convs-1):\n",
    "        net.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "        net.append(nn.ReLU(True))\n",
    "    net.append(nn.MaxPool2d(2,2))#最后最大池化,长度宽度要减半\n",
    "    net_real=nn.Sequential(*net) # nn.Sequential(*list)就是解包操作.因为nn.Sequential()不能接受list,只能将list解包再送入nn.Sequential()中.\n",
    "    return  net_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_stack(num_convs,channels):\n",
    "    net=[]\n",
    "    for num,channel in zip(num_convs,channels):\n",
    "        input_channel=channel[0]\n",
    "        output_channel=channel[1]\n",
    "        net.append(vgg_block(num,input_channel,output_channel))\n",
    "        net_real=nn.Sequential(*net)\n",
    "    return net_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_net = vgg_stack((1, 1, 2, 2, 2), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))\n",
    "# print(vgg_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "test_x = Variable(torch.zeros(1, 3, 256, 256))\n",
    "test_y = vgg_net(test_x)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg,self).__init__()\n",
    "        self.feature=vgg_net\n",
    "        self.fc=nn.Sequential(nn.Linear(512,100),\n",
    "                             nn.ReLU(True),\n",
    "                             nn.Linear(100,10))\n",
    "    def forward(self,x):\n",
    "        x=self.feature(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
    "    x = x.transpose((2, 0, 1)) # 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "     \n",
    "train_set = CIFAR10('./data', train=True, transform=data_tf,download=True)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_set = CIFAR10('./data', train=False, transform=data_tf,download=True)\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3031342105792305\n",
      "2.3030203254631414\n",
      "2.302961255888195\n",
      "2.3026441344824593\n",
      "2.141859237037961\n",
      "1.7751500028783402\n",
      "1.53880198517114\n",
      "1.329759924384334\n",
      "1.1291363163829764\n",
      "0.9430304761128048\n",
      "0.7874331894280661\n",
      "0.6547647973384394\n",
      "0.5450474916173674\n",
      "0.4433809178678886\n",
      "0.3510578895640343\n",
      "0.2852087345098138\n",
      "0.22053750786844575\n",
      "0.17837492057391444\n",
      "0.14442929543573838\n",
      "0.11171062031517857\n"
     ]
    }
   ],
   "source": [
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total\n",
    "def train(net, train_data, test_data, epoch, optimizer, criterion):\n",
    "    if torch.cuda.is_available():\n",
    "        net=net.cuda()\n",
    "    for e in range(epoch):\n",
    "        train_loss=0\n",
    "        train_acc=0\n",
    "        net=net.train()\n",
    "        for im,label in train_data:\n",
    "            if torch.cuda.is_available():\n",
    "                im=Variable(im.cuda())\n",
    "                label=Variable(label.cuda())      \n",
    "            else:\n",
    "                im=Variable(im)\n",
    "                label=Variable(label)\n",
    "            #forward\n",
    "            output=net.forward(im)#此处有问题....\n",
    "            loss=criterion(output,label)\n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            train_acc+=get_acc(output,label)\n",
    "        print(train_loss/len(train_data))\n",
    "\n",
    "net = vgg()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(net, train_data, test_data, 20, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
