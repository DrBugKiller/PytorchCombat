{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 公式及原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch normalization 的实现非常简单，对于给定的一个 batch 的数据 $B = \\{x_1, x_2, \\cdots, x_m\\}$算法的公式如下\n",
    "\n",
    "$$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一行和第二行是计算出一个 batch 中数据的均值和方差，接着使用第三个公式对 batch 中的每个数据点做标准化，$\\epsilon$ 是为了计算稳定引入的一个小的常数，通常取 $10^{-5}$，最后利用权重修正得到最后的输出结果，非常的简单，下面我们可以实现一下简单的一维的情况，也就是神经网络中的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 实现BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before bn:\n",
      " tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]])\n",
      "after bn:\n",
      " tensor([[-1.4142, -1.4142, -1.4142],\n",
      "        [-0.7071, -0.7071, -0.7071],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7071,  0.7071,  0.7071],\n",
      "        [ 1.4142,  1.4142,  1.4142]])\n"
     ]
    }
   ],
   "source": [
    "def simple_batch_norm(x,gamma,beta):\n",
    "    eps=1e-5 #eps denotes epsilon\n",
    "    x_mean=torch.mean(x,dim=0,keepdim=True)#注意dim的取值\n",
    "#     print('x_mean:\\n',x_mean)\n",
    "    x_var=torch.mean((x-x_mean)**2, dim=0, keepdim=True)\n",
    "#     print('x_var:\\n',x_var)\n",
    "    x_hat=(x-x_mean)/torch.sqrt(x_var+eps) #sqrt denotes square root 平方根\n",
    "#     print('x_hat:\\n',x_hat)\n",
    "    return gamma.view_as(x_mean)*x_hat + beta.view_as(x_mean)#如果两个tensor的shape不同,也可以相乘,但前提是dim要相同.\n",
    "\n",
    "#我们来验证一下是否对于任意的输入，输出会被标准化\n",
    "x=torch.arange(15).view(5,3).float()\n",
    "gamma=torch.ones(x.shape[1])\n",
    "beta=torch.zeros(x.shape[1])\n",
    "print('before bn:\\n',x)\n",
    "y=simple_batch_norm(x,gamma,beta)\n",
    "print('after bn:\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这个时候会出现一个问题，就是测试的时候该使用批标准化吗？**\n",
    "\n",
    "答案是肯定的，因为训练的时候使用了，而测试的时候不使用肯定会导致结果出现偏差，但是测试的时候如果只有一个数据集，那么均值不就是这个值，方差为 0 吗？这显然是随机的，所以测试的时候不能用测试的数据集去算均值和方差，而是用训练的时候算出的移动平均均值和方差去代替.\n",
    "\n",
    "### 2. 能够区分训练状态和测试状态的批标准化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before bn:\n",
      " tensor([[ 0.,  1.,  2.],\n",
      "        [ 3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.],\n",
      "        [ 9., 10., 11.],\n",
      "        [12., 13., 14.]], device='cuda:0')\n",
      "cuda:0\n",
      "after bn:\n",
      " tensor([[-1.4142, -1.4142, -1.4142],\n",
      "        [-0.7071, -0.7071, -0.7071],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.7071,  0.7071,  0.7071],\n",
      "        [ 1.4142,  1.4142,  1.4142]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def batch_norm(x,gamma,beta,is_training,moving_mean,moving_var,moving_momentum=0.1):\n",
    "    eps=1e-5\n",
    "    x_mean=torch.mean(x,dim=0,keepdim=True)\n",
    "    x_var=torch.mean((x-x_mean)**2,dim=0,keepdim=True)\n",
    "    if is_training:\n",
    "        x_hat=(x-x_mean)/torch.sqrt(x_var+eps)\n",
    "        moving_mean[:]=moving_momentum*moving_mean+(1.-moving_momentum)*x_mean\n",
    "        moving_var[:]=moving_momentum*moving_var+(1.-moving_momentum)*x_var\n",
    "    else:\n",
    "        x_hat=(x-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    return gamma.view_as(x_mean)*x_hat+beta.view_as(x_mean)\n",
    "#我们来验证一下是否对于任意的输入，输出会被标准化\n",
    "x=torch.arange(15).view(5,3).float().cuda()\n",
    "gamma=torch.ones(x.shape[1]).cuda()\n",
    "beta=torch.zeros(x.shape[1]).cuda()\n",
    "print('before bn:\\n',x)\n",
    "is_training=True\n",
    "moving_mean=torch.zeros(x.shape[1]).cuda()\n",
    "moving_var=torch.zeros(x.shape[1]).cuda()\n",
    "print(beta.device)\n",
    "y=batch_norm(x,gamma,beta,is_training,moving_mean,moving_var,moving_momentum=0.1)\n",
    "print('after bn:\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.使用上一节课将的深度神经网络分类 mnist 数据集的例子来试验一下批标准化是否有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import mnist # 导入 pytorch 内置的 mnist 数据\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用内置函数下载 mnist 数据集\n",
    "train_set = mnist.MNIST('../data', train=True)\n",
    "test_set = mnist.MNIST('../data', train=False)\n",
    "\n",
    "def data_tf(x):\n",
    "    x = np.array(x, dtype='float32') / 255\n",
    "    x = (x - 0.5) / 0.5 # 数据预处理，标准化\n",
    "    x = x.reshape((-1,)) # 拉平\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "train_set = mnist.MNIST('../data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('../data', train=False, transform=data_tf, download=True)\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.315558, Train Acc: 0.911497, Valid Loss: 0.197100, Valid Acc: 0.941555, \n",
      "Epoch 1. Train Loss: 0.175175, Train Acc: 0.949877, Valid Loss: 0.142647, Valid Acc: 0.958366, \n",
      "Epoch 2. Train Loss: 0.135746, Train Acc: 0.961521, Valid Loss: 0.121527, Valid Acc: 0.965585, \n",
      "Epoch 3. Train Loss: 0.112858, Train Acc: 0.967917, Valid Loss: 0.110114, Valid Acc: 0.968948, \n",
      "Epoch 4. Train Loss: 0.097950, Train Acc: 0.971782, Valid Loss: 0.107418, Valid Acc: 0.968849, \n",
      "Epoch 5. Train Loss: 0.085791, Train Acc: 0.975113, Valid Loss: 0.104817, Valid Acc: 0.970332, \n",
      "Epoch 6. Train Loss: 0.076986, Train Acc: 0.977279, Valid Loss: 0.094231, Valid Acc: 0.972508, \n",
      "Epoch 7. Train Loss: 0.069275, Train Acc: 0.979544, Valid Loss: 0.097845, Valid Acc: 0.969937, \n",
      "Epoch 8. Train Loss: 0.063007, Train Acc: 0.981943, Valid Loss: 0.095536, Valid Acc: 0.971816, \n",
      "Epoch 9. Train Loss: 0.057041, Train Acc: 0.983575, Valid Loss: 0.089297, Valid Acc: 0.973596, \n"
     ]
    }
   ],
   "source": [
    "def batch_norm(x,gamma,beta,is_training,moving_mean,moving_var,moving_momentum=0.1):\n",
    "    #batch_norm中间的操作最好在cpu上进行,输出的时候再放到GPU上.\n",
    "    x=x.cpu()\n",
    "    gamma=gamma.cpu()\n",
    "    beta=beta.cpu()\n",
    "    moving_mean=moving_mean.cpu()\n",
    "    moving_var=moving_var.cpu()\n",
    "    \n",
    "    \n",
    "    eps=1e-5\n",
    "    x_mean=torch.mean(x,dim=0,keepdim=True)\n",
    "    x_var=torch.mean((x-x_mean)**2,dim=0,keepdim=True)\n",
    "    if is_training:\n",
    "        x_hat=(x-x_mean)/torch.sqrt(x_var+eps)\n",
    "        \n",
    "        moving_mean[:]=moving_momentum*moving_mean+(1.-moving_momentum)*x_mean\n",
    "        moving_var[:]=moving_momentum*moving_var+(1.-moving_momentum)*x_var\n",
    "    else:\n",
    "        x_hat=(x-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    x_bn=gamma.view_as(x_mean)*x_hat+beta.view_as(x_mean)\n",
    "    \n",
    "    x_bn=x_bn.cuda()\n",
    "    return x_bn\n",
    "\n",
    "# 构建模型\n",
    "class multi_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(multi_network,self).__init__()\n",
    "        self.layer1=nn.Linear(784,100)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.layer2=nn.Linear(100,10)\n",
    "        \n",
    "        self.gamma=nn.Parameter(torch.randn(100))\n",
    "        self.beta=nn.Parameter(torch.randn(100))\n",
    "        self.moving_mean=Variable(torch.zeros(100))\n",
    "        self.moving_var=Variable(torch.zeros(100))\n",
    "    def forward(self,x,is_train=True):\n",
    "        x=self.layer1(x)#[64,100]\n",
    "        x=batch_norm(x,self.gamma,self.beta,is_train,self.moving_mean,self.moving_var)\n",
    "        x=self.relu(x)\n",
    "        x=self.layer2(x)\n",
    "        return x\n",
    "\n",
    "net = multi_network()\n",
    "# 定义 loss 函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1\n",
    "\n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().item()\n",
    "    return num_correct / total\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n",
    "    if torch.cuda.is_available():#如果有GPU()\n",
    "        net = net.cuda()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        net = net.train()\n",
    "        for im, label in train_data:\n",
    "            if torch.cuda.is_available():\n",
    "                im = Variable(im.cuda())  # (bs, 3, h, w)\n",
    "                label = Variable(label.cuda())  # (bs, h, w)                \n",
    "            else:\n",
    "                im = Variable(im)\n",
    "                label = Variable(label)\n",
    "            # forward\n",
    "            output = net(im)\n",
    "            loss = criterion(output, label)\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += get_acc(output, label)\n",
    "\n",
    "        if valid_data is not None:\n",
    "            valid_loss = 0\n",
    "            valid_acc = 0\n",
    "            net = net.eval()\n",
    "            for im, label in valid_data:\n",
    "                if torch.cuda.is_available():\n",
    "                    im = Variable(im.cuda())\n",
    "                    label = Variable(label.cuda())\n",
    "                else:\n",
    "                    im = Variable(im)\n",
    "                    label = Variable(label)\n",
    "                output = net(im)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc += get_acc(output, label)\n",
    "            epoch_str = ( \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \" % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), valid_loss / len(valid_data),\n",
    "                   valid_acc / len(valid_data)))\n",
    "            print(epoch_str)\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %(epoch, train_loss / len(train_data),train_acc / len(train_data)))\n",
    "            print(epoch_str)\n",
    "        \n",
    "train(net, train_data, test_data, 10, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看到，我们自己实现了 2 维情况的批标准化，对应于卷积的 4 维情况的标准化是类似的，只需要沿着通道的维度进行均值和方差的计算，但是我们自己实现批标准化是很累的，pytorch 当然也为我们内置了批标准化的函数，一维和二维分别是 `torch.nn.BatchNorm1d()` 和 `torch.nn.BatchNorm2d()`，不同于我们的实现，pytorch 不仅将 $\\gamma$ 和 $\\beta$ 作为训练的参数，也将 `moving_mean` 和 `moving_var` 也作为参数进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.卷积网络下试用一下批标准化看看效果, `torch.nn.BatchNorm1d()`和`torch.nn.BatchNorm2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tf(x):\n",
    "    x=np.array(x,dtype='float32')/255\n",
    "    x=(x-0.5)/0.5\n",
    "    x=torch.Tensor(x)\n",
    "    x=x.unsqueeze(0)\n",
    "    return x\n",
    "train_set = mnist.MNIST('../data', train=True, transform=data_tf, download=True) # 重新载入数据集，申明定义的数据变换\n",
    "test_set = mnist.MNIST('../data', train=False, transform=data_tf, download=True)\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 0.171464, Train Acc: 0.949627, Valid Loss: 0.065106, Valid Acc: 0.978343, \n",
      "Epoch 1. Train Loss: 0.066592, Train Acc: 0.978928, Valid Loss: 0.061266, Valid Acc: 0.981903, \n",
      "Epoch 2. Train Loss: 0.051171, Train Acc: 0.983975, Valid Loss: 0.054453, Valid Acc: 0.982694, \n",
      "Epoch 3. Train Loss: 0.043667, Train Acc: 0.986574, Valid Loss: 0.060510, Valid Acc: 0.981112, \n",
      "Epoch 4. Train Loss: 0.038681, Train Acc: 0.987906, Valid Loss: 0.037735, Valid Acc: 0.987836, \n"
     ]
    }
   ],
   "source": [
    "# 使用批标准化\n",
    "class conv_bn_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_bn_net, self).__init__()\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 3, padding=1),#padding=1可以设置.\n",
    "            nn.BatchNorm2d(6),#BN一般在卷积层之后\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.classfy = nn.Linear(400, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.stage1(x)#x=[64,1,28,28]\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classfy(x)\n",
    "        return x\n",
    "\n",
    "net = conv_bn_net()\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1\n",
    "train(net, train_data, test_data, 5, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.测试`nn.Conv2d(input_channel,output_channel,size,padding=)`,`nn.MaxPool2d()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b shape: torch.Size([64, 6, 28, 28])\n",
      "torch.Size([64, 16, 24, 24])\n",
      "torch.Size([64, 16, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(64,1,28,28)\n",
    "conv=nn.Conv2d(1,6,3,padding=1)\n",
    "#第二个数字6代表filter_num,即输出的通道数.第三个数字代表filter_size,3x3.在Torch中,卷积核默认是正方形的,默认stride步长是1.\n",
    "b=conv(a)\n",
    "print('b shape:',b.shape)\n",
    "conv2=nn.Conv2d(6,16,5)\n",
    "c=conv2(b)\n",
    "print(c.shape)\n",
    "maxpool=nn.MaxPool2d(4,2)\n",
    "d=maxpool(c)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展:\n",
    "1. `tensor1.view_as(tensor2)`:按照tensor2的shape,reshape一下tensor1\n",
    "2. `if torch.cuda.is_available(): net = net.cuda()`:将模型放到GPU上."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
